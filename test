---
title: "Recharge Prior"
author: "Ching-Fu Chang"
date: "October 20, 2016"
output:
  pdf_document:
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    fig_caption: yes
    highlight: zenburn
    number_sections: yes
    theme: readable
    toc: yes
---

```{r set-options, echo=F, cache=FALSE,dpi=600}
options(width = 80)
options(digits=3)
```
```{r library,warning=FALSE,message=FALSE,echo=F}
library(ggthemes)
library(ggplot2)
library(knitr)
library(cowplot)
library(reshape2)
library(pander)
library(xlsx)
library(stats)
```

# Data structure
The data are from various precious studies that provide $C_{r}$ or equivalent information. These will consitute the prior distribution of $C_{r}$ for our Mingtang site.
Three columns indicating the *degree of similarity*:  
1. **Geology**: 1 if the bedrock is granite and/or gneiss, 0 otherwise.  
2. **Land use**: 1 if the watershed/basin is forested, 0 otherwise.  
3. **Climate**: 0 if the climate type is significantly different from that at Mingtang (e.g., tropical, alpine, desertic), 1 otherwise.  

Bear in mind that the criteria here are still qualitative.   
Although some quantitative metrics could be used, (e.g., ratio of rainfall intensity to surfacial conductivity, aridity index, topographic wetness index, etc.), the more indices we consider the less studies we can use. This tradeoff should be considered carefully since we only have 15 studies so far.
  
The column *Mean.Cr* is the value we'll use for now, which is extracted from the various $C_{r}$. A hierarchical structure for data assimilation, like the one shown by Karina, might be helpful here, for better representation of the data (better than a point estimate as used now).

```{r load the data, echo=FALSE, results='hold'}
# note that the data is in the form of Cr, where Cr = recharge / precipitation
# setwd('D:/BOX/Box Sync/Thesis/Recharge Prior')
 setwd('C:/Todd/Box Sync/Thesis/Recharge Prior')
Cr <- read.xlsx('Recharge.xlsx',sheetIndex  = 1)
Cr[,c(1,2,7)]
```

# Histogram and fitted kernel for the whole data set

The function `density()` is used to fit kernel density, with an Gaussain smoothing kernel. Due to physical constraints, $C_{r}$ cannot be negative and thus the fitted kernel density is then truncated at zero. The concept is quite easy, and a good example can be found [**here**](http://stats.stackexchange.com/questions/65866/good-methods-for-density-plots-of-non-negative-variables-in-r)

```{r kernel estimation, echo=F, warning=F}
# kernel density estimation
kernel.all <- density(Cr$Mean.Cr, kernel="gaussian",bw=bw.nrd(Cr$Mean.Cr))
# truncate the non-positive part
h <- kernel.all$bw # Compute a bandwidth.
w <- 1 / pnorm(0, mean=Cr$Mean.Cr, sd=h, lower.tail=FALSE) # Compute edge weights.
# The truncated weighted density is what we want.
kernel.all <- density(Cr$Mean.Cr, bw=h, kernel="gaussian", weights=w / length(Cr$Mean.Cr))
kernel.all$y[kernel.all$x < 0] <- 0
# check the sum of kernel; should be close to 1
# sum(kernel.all$y * diff(kernel.all$x)[1])
kernel.all <- data.frame(x=kernel.all$x,y=kernel.all$y,dat='whole')# extract the curve
kernel.all <- kernel.all[which(kernel.all$x>=0),] # discard the <0 part
```
```{r histogram of all data, echo=F, fig.width=8,fig.height=5,dpi=600}
  # histogram and kernel density plot
plot.all <- ggplot(data=Cr, aes(x=Mean.Cr, y=..density..)) + geom_histogram(binwidth=0.025,colour = "black",aes(fill = ..density..)) + scale_fill_gradient("",guide=F, low = "blue", high = "red")+background_grid(major = "y")  + geom_line(data=kernel.all, aes(x=x,y=y), size=2, color='#003399') + labs(x=expression(C[r])) + xlim(0,0.4) + theme(axis.title=element_text(face = "bold.italic", size = 16), axis.text.y=element_text(face='bold'))
```

As shown in the figure above, the histogram and the fitted kernel has the shape somewhat positively skewed. The absolute bounds from the data are 
```{r bounds from all data,echo=F}
min(Cr$Mean.Cr)
max(Cr$Mean.Cr)
```

# Histrogram and fitted kernel at various degree of similarity

## Similar in geology, land use, and climate
If we require strictest similarity level, unfortunately only two sites remain. 

```{r strictly similar,echo=F, fig.width=8,fig.height=5,dpi=600}
# find the strictly similar sites
dummy <- which(Cr$Geology==1 & Cr$Land.use==1 & Cr$Climate==1)
s1<-Cr[dummy,]
s1[,c(1,2,7)]
```

Do not get overwhelmed by the consistency of the two values, because the Japanese study by *Katsuyam et al.* actually reported five values ranging from *0.059* to *0.39*, and obviously the variability is quite large.  
Also note that this is the same as geologically and land-use similar sites.

## Similar in land use
We then investigate forested sites regardless of rock type and climate. Only four sites satisfy this condition, implying that similarity of land use is the limiting factor of number of studies that could be used. 
```{r similar in land use,echo=F}
# find the similar sites
dummy <- which(Cr$Land.use==1 )
s2<-Cr[dummy,]
s2[,c(1,2,7)]
```

A key observation here is that the $C_{r}$ values from *Yang et al. (2009)* and *Takagi (2013)* are consistent. These two sites are forested, but the bedrock types are neither granite nor gneiss, thus not included as strictly similar sites.

## Similar in geology
If we look at sites that have either granite or gneiss as the dominant bedrock, regardless of its climate and land use condition, we will have much more studies to use.  
This is not too different from the histogram and kernel for the whole data set, but it's less positively skewed.

```{r kernel & histogram of geologically similar sites, echo=F,fig.width=8,fig.height=5,dpi=600}
# find the similar sites
dummy <- which(Cr$Geology==1 )
s3<-Cr[dummy,]
s3[,c(1,2,7)]
# kernel density estimation
kernel.geo <- density(s3$Mean.Cr, kernel="gaussian",bw='nrd')
# truncate the non-positive part
h <- kernel.geo$bw # Compute a bandwidth.
w <- 1 / pnorm(0, mean=s3$Mean.Cr, sd=h, lower.tail=FALSE) # Compute edge weights.
# The truncated weighted density is what we want.
kernel.geo <- density(s3$Mean.Cr, bw=h, kernel="gaussian", weights=w / length(s3$Mean.Cr))
kernel.geo$y[kernel.geo$x < 0] <- 0
# check the sum of kernel; should be close to 1
# sum(kernel.geo$y * diff(kernel.geo$x)[1])
kernel.geo <- data.frame(x=kernel.geo$x,y=kernel.geo$y,dat='geology')# extract the curve
kernel.geo <- kernel.geo[which(kernel.geo$x>=0),] # discard the <0 part
  # histogram and kernel density plot
plot.s3 <- ggplot(data=s3, aes(x=Mean.Cr, y=..density..)) + geom_histogram(binwidth=0.025,colour = "black",aes(fill = ..density..)) + scale_fill_gradient("",guide=F, low = "blue", high = "red")+background_grid(major = "y")  + geom_line(data=kernel.geo, aes(x=x,y=y), size=2, color='#003399') + labs(x=expression(C[r])) + xlim(0,0.4)+ theme(axis.title=element_text(face = "bold.italic", size = 16), axis.text.y=element_text(face='bold'))
```

## Similar in geology and climate
Now we look at geologically similar sites, but exclude the sites that have climate significantly different from that at Mingtang. Such climatically different sites include Swiss Apls, where the primary water input to the watershed is snow rather than rain, and Peninsular Malaysia, where it's perrenially hot and humid.

```{r kernel & histogram of geologically and climatically similar sites,echo=F,fig.width=8,fig.height=5,dpi=600}
# find the similar sites
dummy <- which(Cr$Geology==1 & Cr$Climate==1 )
s4<-Cr[dummy,]
s4[,c(1,2,7)]
# kernel density estimation
kernel.geocli <- density(s4$Mean.Cr, kernel="gaussian",bw='nrd')
# truncate the non-positive part
h <- kernel.geocli$bw # Compute a bandwidth.
w <- 1 / pnorm(0, mean=s4$Mean.Cr, sd=h, lower.tail=FALSE) # Compute edge weights.
# The truncated weighted density is what we want.
kernel.geocli <- density(s4$Mean.Cr, bw=h, kernel="gaussian", weights=w / length(s4$Mean.Cr))
kernel.geocli$y[kernel.geocli$x < 0] <- 0
# check the sum of kernel; should be close to 1
# sum(kernel.geo$y * diff(kernel.geo$x)[1])
kernel.geocli <- data.frame(x=kernel.geocli$x,y=kernel.geocli$y,dat='geo & climate')# extract the curve
kernel.geocli <- kernel.geocli[which(kernel.geocli$x>=0),] # discard the <0 part
  # histogram and kernel density plot
plot.s4 <- ggplot(data=s4, aes(x=Mean.Cr, y=..density..)) + geom_histogram(binwidth=0.025,colour = "black",aes(fill = ..density..)) + scale_fill_gradient("",guide=F, low = "blue", high = "red")+background_grid(major = "y") +  geom_line(data=kernel.geocli, aes(x=x,y=y), size=2, color='#003399') + labs(x=expression(C[r])) + xlim(0,0.4) +theme(axis.title=element_text(face = "bold.italic", size = 16), axis.text.y=element_text(face='bold'))
```

# Concluding Remark
The remaining question is **which data set we should use**.  
The answer depends on the aspect from which we look at the recharge prior. If we want bounds of plausible $C_{r}$ values, we should look at ranges of the subsets with various similarity constraints, as shown in the following figure.

```{r range, message=F,echo=F, fig.width=8,fig.height=4,dpi=600}
# range of subsets
rg <- data.frame(w=range(Cr$Mean.Cr),
                 s=range(s1$Mean.Cr),
                 l=range(s2$Mean.Cr),
                 g=range(s3$Mean.Cr),
                 gc=range(s4$Mean.Cr))
colnames(rg) <- c('whole','geo & land','land use','geology','geo & climate')
# melt it
rg <- melt(rg)
# plot it
rgplot<-ggplot(data=rg,aes(y=variable,x=value) ) + geom_line(size=1.5,color='#00CC66')  + labs(y='',x=expression(C[r]))  + theme(axis.title=element_text(face = "bold.italic", size = 16), axis.text.y=element_text(face='bold')) + geom_text(aes(label=value), parse=T, size=4)
rgplot
```

If we are looking for the prior distribution, then the histograms and the estimated kernel densities are the ones we should investigate, shown as follows.

```{r histograms and kernels,echo=F, fig.width=8,fig.height=5,dpi=600}
plot_grid(plot.all,plot.s3,plot.s4,nrow=3,labels = c('Whole Data Set','Geologically Similar','Geologically & Climatically Similar'))
```

```{r combine the plots, eval=F,echo=F}
# range of subsets
rg <- data.frame(w=range(Cr$Mean.Cr),
                 g=range(s3$Mean.Cr),
                 gc=range(s4$Mean.Cr))
colnames(rg) <- c('whole','geo','geo & climate')
# melt it
rg <- melt(rg)
# plot it
rgplot<-ggplot(data=rg,aes(y=variable,x=value) ) + geom_line(size=1.5,color='#00CC66') + labs(y='',x=expression(C[r]))  + theme(axis.title=element_text(face = "bold.italic", size = 16), axis.text.y=element_text(face='bold')) + geom_text(aes(label=value), parse=T,size=4)

plot_grid(plot.all,plot.s3,plot.s4,rgplot,nrow=2,labels = c('(a) Whole Data Set','(b) Geologically Similar','(c) Geologically & Climatically Similar','(d) Ranges of Data'))
```

# After story: comparison with SWAT
We can compare the results with the $C_{r}$ values obtained by running the SWAT model. I basically use the daily weather data (Huoshan, Luan, Tongchen) and the spatial data from [WATER-BASE](http://inweh.unu.edu/waterbase/).

```{r compare w/ SWAT, echo=F}
# construct the function for calculating recharge ratio
# the working directory must be set properly
RR <- function(file,pos=c(7,16))
{
  # read hru output; note that we prepared outputhru.txt beforehand by using "save as"
  r <- read.table(file=file,skip=9,header=F)
  nHRU <- max(r[,2])
  # the parts we want, precipitation and groundwater recharge
  r<-r[,pos]
  # number of HRUs = nHRU
  # generate time series
  t <- seq(from=as.Date('2016-01-01'), by='month', length.out = nrow(r)/nHRU )
  
  # organizing the data into the form we want
  prcp <- rchg <- data.frame(matrix(ncol=nHRU,nrow=length(t)))
  
  for (j in 1:length(t))
  {
    prcp[j,] <- r[ seq(((j-1)*nHRU+1) , j*nHRU)  ,1]
    rchg[j,] <- r[ seq(((j-1)*nHRU+1) , j*nHRU)  ,2]
  }
  
  # add time
  prcp <- data.frame(time=t,prcp);rchg<-data.frame(time=t,rchg)
  
  # compute recharge ratio (total recharge over total precipitation)
  rr <- apply(rchg[,c(2:ncol(rchg))],2,mean)  / apply(prcp[,c(2:ncol(prcp))],2,mean)
  rr <-data.frame(rr)
  row.names(rr) <- paste('HRU',as.character(1:nHRU))
  
  return(rr)
}

# read recharge ratio -----
file="C:/Todd/Box Sync/SWAT/Mingtang/MT90/Scenarios/Default/TxtInOut/output.hru"
rr <- RR(file)
summary(rr)


# kernel for SWAT output
kernel.SWAT <- density(rr$rr, kernel="gaussian",bw=bw.nrd(rr$rr))
# truncate the non-positive part
h <- kernel.SWAT$bw # Compute a bandwidth.
w <- 1 / pnorm(0, mean=rr$rr, sd=h, lower.tail=FALSE) # Compute edge weights.
# The truncated weighted density is what we want.
kernel.SWAT <- density(rr$rr, bw=h, kernel="gaussian", weights=w / length(rr$rr))
kernel.SWAT$y[kernel.SWAT$x < 0] <- 0
# check the sum of kernel; should be close to 1
# sum(kernel.geo$y * diff(kernel.geo$x)[1])
kernel.SWAT <- data.frame(x=kernel.SWAT$x,y=kernel.SWAT$y,dat='SWAT')# extract the curve
kernel.SWAT <- kernel.SWAT[which(kernel.SWAT$x>=0),] # discard the <0 part

# combine all the kernels
kernels <- rbind(kernel.all,kernel.geo,kernel.geocli,kernel.SWAT)

# ggplot it
ggplot(data=rr, aes(x=rr, y=..density..)) + geom_histogram(binwidth=0.01,colour = "black",aes(fill = ..density..)) + scale_fill_gradient2("",guide=F)+background_grid(major = "y")   + geom_line(data=kernels, aes(x=x,y=y,color=dat), size=2) + labs(x=expression(C[r])) + xlim(0,0.4) +theme(axis.title=element_text(face = "bold.italic", size = 16), axis.text.y=element_text(face='bold'), legend.position='top') + scale_color_discrete('Kernel Estimate',labels=c('(a)','(b)','(c)','SWAT')) 
```

```{r stats summary of data subsets and SWAT output, echo=F}
# here I want to know the moments of each estimated kernel densities
stat <- function(a, N=1e6)
{
  library(moments)
  # note that Gaussian kernel is used here, and the bandwidth method is nrd
  d <-  density(a, kernel="gaussian",bw='nrd',from=0)
  s <- rnorm(N, mean = sample(a,N,replace = T), sd = d$bw)
  # discard negative parts
  s[which(s<0)] <- NA
  # quantiles
  summ <- summary(s,na.rm=T)[-c(4,7)]
  # mean and mode
  me <- mean(s,na.rm=T)
  mo <- d$x[which(d$y == max(d$y))]
  # standard deviation
  v <- sd(s,na.rm=T)
  # skewness
  sk <- skewness(s,na.rm=T)
  # kurtosis
  kt <- kurtosis(s,na.rm=T)
  output <- c(summ,me,mo,v,sk,kt)
  names(output)[6:10] <- c('mean','mode','std','skewness','kurtosis')
  return(output)
}
stats <- data.frame(whole=stat(Cr$Mean.Cr),geology=stat(s3$Mean.Cr),gc=stat(s4$Mean.Cr),SWAT=stat(rr$rr))
colnames(stats)[3] <- 'geo & climate'
round(stats,5)
# save the computed stats
# write.csv(stats,'D:/BOX/Box Sync/Thesis/Recharge Prior/Cr stats.csv')
 write.csv(round(stats,3),'C:/Todd/Box Sync/Thesis/Recharge Prior/Cr stats.csv')
```

A kernel density estimate is a mixture distribution; for every observation, there's a kernel. If the kernel is a scaled density, this leads to a simple algorithm for sampling from the kernel density estimate:  
* sample (with replacement) a random observation from the data  
* sample from the kernel, and add the previously sampled random observation  

If (for example) you used a Gaussian kernel, your density estimate is a mixture of 100 normals, each centred at one of your sample points and all having standard deviation hh equal to the estimated bandwidth.  
See [this link](http://stats.stackexchange.com/questions/82797/how-to-draw-random-samples-from-a-non-parametric-estimated-distribution)
